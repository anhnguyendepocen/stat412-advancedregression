---
title: "Stats 412 Homework 3"
author: 'James Lepore (UID: 003933437)'
date: "5/25/2018"
output: html_document
highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(gbm)
library(party)
library(caret)
library(data.table)
library(mgcv)
library(mda)
library(e1071)
library(earth)
```
  
### Music Genre
This data set was published as a contest data set on the TunedIT web site (http://tunedit.org/challenge/music-retrieval/genres). In this competition, the objective was to develop a predictive model for classifying music into six categories. In total, there were 12,495 music samples for which 191 characteristics were determined. All predictors were continuous; many were highly correlated and the predictors spanned different scales of measurement. This data collection was created using 60 performers from which 15–20 pieces of music were selected for each performer. Then 20 segments of each piece were parameterized in order to create the final data set. Hence, the samples are inherently not independent of each other.

```{r}
library(readr)
genres <- read_csv("https://raw.githubusercontent.com/natelangholz/stat412-advancedregression/master/week-7/problem-set-3/genresTrain.csv")
```

### 1
**Random Forest**: Fit a random forest model using both CART trees and conditional inference trees to the music genre predictors. What are the differences in the models? Do you have any difficulty on the full data set? 

Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. 

```{r}
set.seed(49)

tenFoldCV <- createFolds(genres, k=10, returnTrain=TRUE)

tunegrid = floor(expand.grid(.mtry=c(sqrt(length(genres))/3, 
                                      sqrt(length(genres))/2, 
                                      sqrt(length(genres))/1.5,
                                      seq(sqrt(length(genres)), 
                                        sqrt(length(genres))*12, 
                                        sqrt(length(genres))))))
```

```{r, eval=FALSE}
rfFit <- train(GENRE ~ .,
               data = genres,
               method = "rf",
               preProc = c("center", "scale"),
               tuneGrid = tunegrid,
               trControl = trainControl(method = "cv", 
                                        index = tenFoldCV,
                                        search = "grid",
                                        verboseIter = TRUE))

save(rfFit, file="week-7/problem-set-3/rfFit.RData")

crfFit <- train(GENRE ~ .,
                data = genres,
                method = "cforest",
                preProc = c("center", "scale"),
                tuneGrid = tunegrid,
                trControl = trainControl(method = "cv", 
                                         index = tenFoldCV,
                                         search = "grid",
                                         verboseIter = TRUE))

save(crfFit, file="week-7/problem-set-3/crfFit.RData")
```

```{r}
load("week-7/problem-set-3/rfFit.RData")

rfFit
plot(rfFit)

load("week-7/problem-set-3/crfFit.RData")

crfFit
plot(crfFit)
```

We see that the random forest built with regular classification trees seemed to perform better in terms of accuracy than the one built using conditional inference trees. The conditional inference trees took substantially longer to run, and the storage for the `caret` train results was around 2 GB compared to the 45 MB of the regular random forest. The conditional inference trees use a different algorithm to choose which parameters are kept which, among other things, helps when predictor variables are correlated with each other. The conditional inference trees found maximum accuracy at 27 parameters selected, whereas the regular trees was at 41. 

### 2
**Data Splitting**: What data splitting method(s) would you use for these data? Explain.

```{r}
table(genres$GENRE)
prop.table(table(genres$GENRE))
barplot(prop.table(table(genres$GENRE)))
```

We can see by looking at the distribution of genres in the data that they are not all equally as likely. To ensure that we train our model using splits of the data that contain similar distributions of genres, we can use the `createFolds` function in the `caret` package (seen in the code above). We can see below that each of the folds has a similar percentage of each genre as the full dataset. 

```{r}
for (i in 1:10) {
  print(prop.table(table(genres[tenFoldCV[[i]],]$GENRE)))
}
```

### 3
**Variable Importance**: Create a variable importance plot from you best model. What features are most important?

```{r}
varImp(rfFit)
plot(varImp(rfFit), top = 20)
```

We see here the top 20 most important factors for the regular random forest model. It is possible that because of the correlation of the predictors, and the fact that the observations are not independent of each other, that the conditional random forest is actually the better fit and that the regular random forest overestimates the accuracy. We see below that several variables are basically perfectly correlated with each other.

```{r}
data = as.data.frame(as.table(cor(genres[,1:191])))
combinations = combn(colnames(genres[,1:191]), 2, FUN = function(x) {paste(x, collapse="_")})
data = data[data$Var1!=data$Var2, ]
data = data[paste(data$Var1, data$Var2, sep="_") %in% combinations, ]
head(data[order(-abs(data$Freq)),], 30)
```

  * * *
  
### Simulated data
Friedman introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

$$y = 10\sin(πx_1x_2)+20(x_3 −0.5)^2 +10x_4 +5x_5 +N(0,σ^2)$$
where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called `mlbench.friedman1` that simulates these data.

```{r rf}
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"

simulated.copy <- simulated
```
  
### 4
**Random Forest and Variable Importance**:  Fit a random forest model to all of the predictors, then estimate the variable importance scores. Did the random forest model significantly use the uninformative predictors (V6 – V10)?

```{r}
rfFit.2 <- train(y ~ .,
                  data = simulated,
                  method = "rf",
                  importance = TRUE)

rfFit.2
varImp(rfFit.2)
plot(varImp(rfFit.2))
```

As we would expect, we see that V6-V10 are the five least important predictors.

### 5
**Correlated Predictor**:Now add an additional predictor that is highly correlated with one of the informative predictors. For example:

```{r cor}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
```

Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?

```{r}
rfFit.2 <- train(y ~ .,
                  data = simulated,
                  method = "rf",
                  importance = TRUE)

rfFit.2
varImp(rfFit.2)
plot(varImp(rfFit.2))
```

When we add in a predictor that correlates with one of the function inputs (V1 in this case), that importance is brought down and the correlated predictor shows up as being fairly important. Basically, they are splitting the importance because when V1 is not included in the model, most of that can be made up by including the duplicated variable instead and so the relative importance of including the actual input variable is lessened.

```{r}
simulated$duplicate2 <- simulated$V1 + rnorm(200) * .4
cor(simulated$duplicate2, simulated$V1)

rfFit.2 <- train(y ~ .,
                  data = simulated,
                  method = "rf",
                  importance = TRUE)

rfFit.2
varImp(rfFit.2)
plot(varImp(rfFit.2))
```

When we add in yet another variable correlated with V1, the relative importance of including that variable continues to lessen as there are alternatives that explain the variation fairly well to choose from. This time that effect manifests itself in splitting importance between the two duplicates, which are by definition also correlated to each other.

### 6
**Gradient Boosted Machine**: Repeat the process in 5 and 6 with different tree models, such as boosted trees. Does the same pattern occur? 

```{r}
set.seed(33)
gbmFit <- train(y ~ .,
                  data = simulated.copy,
                  method = "gbm", 
                  verbose = FALSE)

varImp(gbmFit)
plot(varImp(gbmFit))
```

We see with the GBM that not only are V6-V10 the 5 predictors with the lowest importance, but the importances are actually much closer to 0. Gradient boosted machines are built in series rather than parallel as random forests are. This means that the trees can learn from the previous trees about what was predicted well and what was not, so predictors that are more highly correlated with the output are given greater importance because the marginal improvements made by using the actual inputs are weighted appropriately. 

```{r}
simulated.copy$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated.copy$duplicate1, simulated$V1)

gbmFit <- train(y ~ .,
                  data = simulated.copy,
                  method = "gbm", 
                  verbose = FALSE)

varImp(gbmFit)
plot(varImp(gbmFit))
```

```{r}
simulated.copy$duplicate2 <- simulated$V1 + rnorm(200) * .4
cor(simulated.copy$duplicate2, simulated$V1)

gbmFit <- train(y ~ .,
                  data = simulated.copy,
                  method = "gbm", 
                  verbose = FALSE)

varImp(gbmFit)
plot(varImp(gbmFit))
```

The importance of V1 does decrease a bit, but not nearly to the extent that we saw with the random forests. The correlated variables are appropriately more important than the uninformative predictors, but still very low considered to the rest of the actual inputs.

### Pulling Punches Part 2

The two `.Rdata` files under week 7 come as an abbreviated version of punch profiles from a boxing system to measure acceleration of boxers during live fights. The `gridded` list from the second file below has each individual punch profile which has a list item being a 3 column data frame of time (in ms around the middle of the punch event), acceleration in x (forward-back in g's), and acceleration in y (side-to-side in g's). Also attached are some other fields which are of less importance and contribute to this being a somewhat messy data set.

```{r}
set.seed(532)

load(file = 'week-7/problem-set-3/boxer_features_and_force.Rdata')
load(file = 'week-7/problem-set-3/force_punch_profiles.Rdata')
```

There are 1000 punch profiles each with an associated force (in Newtons) and boxer who threw the punch. Use the `ff` data frame as ground truth for punch force (variable =`force`) in addition to the rest of the boxer information. Other boxer information included variables around their size to be a proxy for 'effective mass'.

```{r, eval=FALSE}
N = length(gridded)
profLong <- data.frame(matrix(ncol = 4, nrow = 1001*N))

for (i in 1:N) {
 
 if (i==1) {
   profLong <- cbind(rep(i, 1001), gridded[[i]]$profile)
 }
 else {
   profLong <- rbind(profLong,
                     cbind(rep(i, 1001),
                           gridded[[i]]$profile))
 }
 
}

profLong <- as.data.table(profLong)

save(profLong, file="profLong.RData")
```

```{r, eval=FALSE}
load(file="week-7/problem-set-3/profLong.RData")

N = max(profLong$V1)
features <- data.frame(matrix(ncol = 45, nrow = N))

for (i in 1:N) {
 
 temp.row <- c(i, max(profLong[V1==i,]$V3))
 temp.row <- c(temp.row, max(profLong[V1==i,]$V4))
 temp.row <- c(temp.row, min(profLong[V1==i,]$V3))
 temp.row <- c(temp.row, min(profLong[V1==i,]$V4))
 temp.gam <- gam(V3 ~ s(V2, k = 20, bs = "cs"),
                 data = profLong[V1==i,])
 temp.row <- c(temp.row, temp.gam$coefficients)
 temp.gam <- gam(V4 ~ s(V2, k = 20, bs = "cs"),
                 data = profLong[V1==i,])
 temp.row <- c(temp.row, temp.gam$coefficients)
 
 if (i==1) features <- temp.row
 else features <- rbind(features, temp.row)
 
}

save(features, file="week-7/problem-set-3/features.RData")
```

### 7
**Estimations**: Use features (and/or new features) created from your problem set 2 to estimate punch force using a MARS model. Use RMSE as your error estimate

```{r}
load(file="week-7/problem-set-3/features.RData")

fulldata <- cbind(features, ff$force)
fulldata <- fulldata[, -c(1)]
colnames(fulldata) <- paste0("V", 1:45)

marsFit <- train(V45 ~ .,
                  data = fulldata,
                  method = "bagEarth")

marsFit
```

### 8
**Estimations improved** Now try a few different (gbm, randomForest) models through the `caret` package and different data splitting techniques to compare. Comparing RMSE which model performs the best?

```{r}
rfFit.3 <- train(V45 ~ .,
                  data = fulldata,
                  method = "rf")

rfFit.3

gbmFit.3 <- train(V45 ~ .,
                    data = fulldata,
                    method = "gbm",
                    verbose = FALSE)

gbmFit.3

svmFit <- train(V45 ~ .,
                data = fulldata,
                method = "svmRadial")

svmFit
```

We see that the SVM produces the lowest RMSE and the MARS model produces the highest (SVM > RF > GBM > MARS). Furthermore, we can try different splitting techniques on the SVM.

```{r}
svmFitB632 <- train(V45 ~ .,
                    data = fulldata,
                    method = "svmRadial",
                    trControl = trainControl(method = "boot632", 
                                              number = 50))

svmFitB632

svmFitCV <- train(V45 ~ .,
                  data = fulldata,
                  method = "svmRadial",
                  trControl = trainControl(method = "cv", 
                                            number = 10))

svmFitCV

svmFitLGO <- train(V45 ~ .,
                    data = fulldata,
                    method = "svmRadial",
                    trControl = trainControl(method = "LGOCV", 
                                              number = 50,
                                              p = .8))

svmFitLGO
```

The 632 bootstap method for splitting the data even further improves the SVM RMSE (BS632 > CV10 > LGO.8). 
